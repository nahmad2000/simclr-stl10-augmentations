# configs/base_simclr.yaml
# Base configuration shared across all SimCLR experiments

model:
  name: simclr             # Model identifier
  backbone: resnet18       # Encoder architecture
  projection_dim: 128      # Output dimension of the projection head

data:
  dataset: stl10           # Dataset name
  data_dir: ./data         # Path to dataset root
  image_size: 96           # Target image size
  num_workers: 8           # Number of dataloader workers (adjust based on system)

# --- Augmentation Defaults (Specific configs will override 'enabled' flags) ---
augmentations:
  random_resized_crop:
    size: 96               # Corresponds to data.image_size
    scale: [0.2, 1.0]      # Standard scale range for SimCLR crop
  random_horizontal_flip:
    p: 0.5                 # Standard probability
  color_jitter:
    enabled: false         # Disabled by default
    brightness: 0.4
    contrast: 0.4
    saturation: 0.4
    hue: 0.1
    p: 0.8                 # Probability of applying jitter if enabled
  gaussian_blur:
    enabled: false         # Disabled by default
    kernel_size: 9         # Kernel size (must be odd)
    sigma: [0.1, 2.0]      # Sigma range
    p: 0.5                 # Probability of applying blur if enabled
  grayscale:
    enabled: false         # Disabled by default
    p: 0.2                 # Probability of applying grayscale if enabled

# --- Pretraining Hyperparameters ---
pretrain:
  optimizer: adam           # CHANGED: from sgd to adam
  learning_rate: 0.0003     # CHANGED: from 0.05 to 0.0003 (suitable for Adam)
  weight_decay: 1e-4       # Keep as is
  # momentum: 0.9          # REMOVED: Not applicable to Adam
  lr_schedule: cosine      # Keep as is
  epochs: 300              # Keep as is (benchmark used 100, but let's keep your setting for now)
  batch_size: 256          # Keep as is
  temperature: 0.07        # CHANGED: from 0.1 to 0.07 (matches reference)
  n_views: 2               # Keep as is
  fp16_precision: false    # Keep as is
  early_stopping:
    enabled: false          # Keep as is (consider disabling temporarily for direct epoch comparison)
    # patience: 20           # Keep as is
    # min_delta: 0.001       # Keep as is

# --- Evaluation Settings ---
evaluation:
  linear_probe:
    epochs: 100            # Keep as is
    optimizer: adam        # CHANGED: from sgd to adam
    learning_rate: 0.0003  # CHANGED: from 0.1 to 0.0003 (matches reference eval)
    # momentum: 0.9        # REMOVED: Not applicable to Adam
    weight_decay: 0.0008   # ADDED: Weight decay used in reference eval
    batch_size: 256        # Keep as is
  knn:
    k: 10                  # Number of neighbors for k-NN eval
    metric: cosine         # Distance metric for k-NN

# --- Output & Logging ---
output:
  results_dir: ./results   # Root directory for saving results
  # run_dir: Will be generated automatically: {results_dir}/{model_name}_{aug_name}
  checkpoint_name: model_checkpoint.pth # Name for saved model checkpoints
  train_loss_csv: training_loss_{model}_{aug}.csv # Naming convention for loss log
  linear_acc_txt: linear_probe_acc_{model}_{aug}.txt # Naming for linear probe result
  knn_acc_txt: knn_acc_{model}_{aug}.txt # Naming for k-NN result
  log_name: training.log   # Name for the text log file within the run_dir